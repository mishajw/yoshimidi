tag = "2023-12-03_v4"
use_wandb = true

[transformer]
type = "gpt2"
num_layers = 6
residual_stream_size = 512
num_attention_heads = 8
context_window = 1024

[training]
batch_size = 32
learning_rate = 1e-4
device = "cuda"
dtype = "float32"
[training.metrics_schedule]
every_n_steps = 5e2  # every 1%

[checkpoint.schedule]
# We dedicate 4GB of disk space to checkpoints (16GB total - 8GB dataset - 4GB buffer).
# We have space for ~16 checkpoints (4GB disk / 240MB checkpoints).
# We run for 5e4 steps, so we save every 3e3 steps.
every_n_steps = 3e3
[checkpoint.rolling_schedule]
every_n_steps = 1e3

[eval]
split = 0.001
batch_size = 32

[eval.schedule]
enable = false
every_n_steps = 500
