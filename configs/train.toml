tag = "2023-11-19_v1"
use_wandb = true

[transformer]
type = "gpt2"
num_layers = 6
residual_stream_size = 512
num_attention_heads = 8
context_window = 1024

[training]
batch_size = 32
learning_rate = 1e-4
device = "cuda"
dtype = "float32"

[checkpoint.schedule]
# We dedicate 4GB of disk space to checkpoints (16GB total - 8GB dataset - 4GB buffer).
# We have space for ~16 checkpoints (4GB disk / 240MB checkpoints).
# We run for 1.6e6 steps, so we save every 1e5 steps.
every_n_steps = 1e5
[checkpoint.rolling_schedule]
every_n_steps = 1e4

[eval]
split = 0.001
batch_size = 32

[eval.schedule]
every_n_steps = 500
